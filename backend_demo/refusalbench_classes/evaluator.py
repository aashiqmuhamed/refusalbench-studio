import litellm
import json
import os
import asyncio
import logging
from typing import Dict, List, Union, Optional
from tenacity import retry, stop_after_attempt, wait_random_exponential

logger = logging.getLogger(__name__)


class AsyncRAGModelEvaluator:
    """
    Class for evaluating RAG models on the RefusalBench dataset with refusal capabilities.
    Implements dual evaluation logic for both answerable and unanswerable instances.
    Uses LiteLLM for model API calls.
    """
    def __init__(self, model_id, evaluator_engine_id, 
                 batch_size=10, max_concurrent=20, temperature=0.1, max_tokens=2000,
                 model_kwargs=None, engine_kwargs=None):
        """
        Initialize the evaluator.
        
        Args:
            model_id: Model ID to evaluate (LiteLLM-compatible string)
            evaluator_engine_id: Model ID to use for evaluation (judge)
            batch_size: Number of examples to process in parallel batches
            max_concurrent: Maximum concurrent model calls
            temperature: Temperature for model generation
            max_tokens: Maximum tokens for model responses
            model_kwargs: Additional kwargs for the evaluator model (e.g. api_base, api_key)
            engine_kwargs: Additional kwargs for the evaluation engine (e.g. api_base, api_key)
        """
        self.model_id = model_id
        self.evaluator_engine_id = evaluator_engine_id
        self.batch_size = batch_size
        self.max_concurrent = max_concurrent

        
        # Model generation parameters - use provided kwargs or build defaults
        if model_kwargs:
            self.model_kwargs = dict(model_kwargs)
        else:
            self.model_kwargs = {
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
        
        # Evaluation engine kwargs (for the judge model)
        self.engine_kwargs = dict(engine_kwargs) if engine_kwargs else {
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        
        # Async processing setup
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # Define valid refusal codes
        self.valid_refusal_codes = [
            "REFUSE_AMBIGUOUS_QUERY",
            "REFUSE_CONTRADICTORY_CONTEXT", 
            "REFUSE_INFO_MISSING_IN_CONTEXT",
            "REFUSE_FALSE_PREMISE_IN_QUERY",
            "REFUSE_GRANULARITY_MISMATCH",
            "REFUSE_NONFACTUAL_QUERY",
            "REFUSE_OTHER"
        ]
    
    @retry(
        wait=wait_random_exponential(min=1, max=10),
        stop=stop_after_attempt(3),
    )
    async def _call_model_async(self, model_id: str, prompt: str) -> str:
        """Call model via LiteLLM asynchronously with retry logic."""
        async with self.semaphore:
            try:
                logger.info(f"Calling model: {model_id}")
                messages = [{"role": "user", "content": prompt}]
                
                # Use engine_kwargs for the judge model, model_kwargs for the evaluator model
                kwargs = self.engine_kwargs if model_id == self.evaluator_engine_id else self.model_kwargs
                
                # Call LiteLLM completion
                response = await litellm.acompletion(
                    model=model_id,
                    messages=messages,
                    **kwargs
                )
                
                result = response.choices[0].message.content
                logger.info(f"Got response from {model_id}: {result[:100]}...")
                return result
                
            except Exception as e:
                logger.error(f"Error calling model {model_id}: {e}")
                raise e
    

    async def get_model_answer(self, model_id, context, query):
        """
        Get answer prediction from a model using the RefusalBench prompt.
        Uses only the first retrieved document as context.
        
        Args:
            model_id: Model ID to query
            context: First context passage only
            query: User query
            
        Returns:
            Raw model response
        """
        # RefusalBench prompt template
        prompt_template = """
        You are an advanced AI assistant designed to answer questions based **only** on the provided context passages. You must not use any external knowledge or prior training. Your goal is to be precise, factual, and aware of the limitations of the provided information.
        
        Read the following Query and Context Passages carefully.
        
        QUERY:
        {query}
        
        CONTEXT PASSAGES:
        {context}
        
        **INSTRUCTIONS:**
        1.  Your primary task is to answer the QUERY if, and only if, a complete and faithful answer can be derived *solely* from the facts within the CONTEXT PASSAGES.
        2.  You are permitted and expected to perform reasoning, including common-sense inference and basic mathematics, as long as it operates *exclusively* on the information provided in the context.
        3.  If a direct and faithful answer is not possible due to one of the issues below, you MUST respond with **only** the corresponding refusal code. Do not add any other text or explanation.
        
        **REFUSAL CODES:**
        
        *   **`REFUSE_AMBIGUOUS_QUERY`**
            *   **When to use:** When the query or context creates multiple plausible interpretations, making a single definitive answer impossible.
            *   **Example:** Query asks about "the bat's weight" after the context mentions both a baseball bat and an animal.
        
        *   **`REFUSE_CONTRADICTORY_CONTEXT`**
            *   **When to use:** When the context contains directly contradictory or logically inconsistent information that is essential for answering the query.
            *   **Example:** One sentence states "revenue was $10M" and another states "revenue was $12M".
        
        *   **`REFUSE_INFO_MISSING_IN_CONTEXT`**
            *   **When to use:** When the context is **silent** on the matter. The specific fact, entity, or value needed to answer is **entirely absent**. Use this when the information is simply not there.
            *   **Example:** Query asks "Who is the CEO?" but the context only describes the company's products.
        
        *   **`REFUSE_FALSE_PREMISE_IN_QUERY`**
            *   **When to use:** When the query assumes a fact (e.g., an event that never happened, a non-existent object) that is contradicted by or unsupported by the context.
            *   **Example:** Query asks "Why did the company's Mars division fail?" but the context states the company has no Mars division.
        
        *   **`REFUSE_GRANULARITY_MISMATCH`**
            *   **When to use:** When the context **contains relevant information**, but it is at the **wrong scale or level of abstraction**. Answering would require a transformation (e.g., aggregation, averaging, generalization) that is impossible or invalid with the given data. This is different from `REFUSE_INFO_MISSING_IN_CONTEXT` because the information is not absent, just incompatible.
            *   **Example:** Query asks for the "average employee salary," but the context only provides individual salaries for two of the fifty employees. The information is present, but insufficient for the requested aggregation.
        
        *   **`REFUSE_NONFACTUAL_QUERY`**
            *   **When to use:** When the query asks for subjective opinions, predictions, hypotheticals, or moral judgments that cannot be objectively derived from the provided facts.
            *   **Example:** Query asks "Is this a good investment?" but the context only provides the stock price.
        
        *   **`REFUSE_OTHER`**
            *   **When to use:** For any other specific issue not covered above that makes a faithful answer impossible.
        
        Provide your response below:
        ANSWER:
        """
        
        prompt = prompt_template.format(query=query, context=context)
        response = await self._call_model_async(model_id, prompt)
        return response.strip()

    async def classify_and_evaluate_response(self, model_output, query, correct_answers=None):
        """
        Universal LLM judge to classify response type and evaluate answer quality.
        Always asks for both classification and quality score - we extract what we need.
        
        Args:
            model_output: Raw model response
            query: User query
            correct_answers: Correct answers (can be None for unanswerable instances)
            
        Returns:
            Tuple of (response_type, answer_quality_score, explanation)
        """
        # Format correct answers (even if None, we'll handle it in the prompt)
        if correct_answers:
            formatted_correct = f'"{correct_answers}"'
        else:
            formatted_correct = "[No reference answers provided - this may be an unanswerable instance]"
        
        # Universal prompt that always asks for both classification and quality
        universal_prompt = f"""Analyze the following model response by performing BOTH classification and quality evaluation:

STEP 1: CLASSIFICATION
Determine if this model response is:
1. An attempt to answer the query â†’ "answer_attempt"  
2. A refusal with a specific code:
   - REFUSE_AMBIGUOUS_QUERY
   - REFUSE_CONTRADICTORY_CONTEXT
   - REFUSE_INFO_MISSING_IN_CONTEXT
   - REFUSE_FALSE_PREMISE_IN_QUERY
   - REFUSE_GRANULARITY_MISMATCH
   - REFUSE_NONFACTUAL_QUERY
   - REFUSE_OTHER

Look for refusal codes even if they appear with additional text.

STEP 2: QUALITY EVALUATION
If the response is an answer attempt AND reference answers are provided, evaluate quality:
- Score 1: Not relevant to query and reference answers
- Score 2: According to reference answers but not relevant to query  
- Score 3: Relevant to query and reference answers but contains mistakes
- Score 4: Relevant and has exact same metrics as reference, but not concise
- Score 5: Relevant to query and fully correct according to reference answers

If the response is a refusal OR no reference answers provided, use "N/A".

QUERY:
{query}

MODEL RESPONSE:
{model_output}

REFERENCE ANSWERS:
{formatted_correct}

Respond in this exact format:
CLASSIFICATION: [answer_attempt OR one of the REFUSE_* codes]
QUALITY_SCORE: [1-5 if answer_attempt with references, otherwise N/A]
EXPLANATION: [brief reasoning for both classification and score]"""

        response = await self._call_model_async(self.evaluator_engine_id, universal_prompt)
        
        # Parse the response
        try:
            lines = response.strip().split('\n')
            
            # Extract classification
            classification_line = [line for line in lines if line.startswith('CLASSIFICATION:')][0]
            classification = classification_line.split('CLASSIFICATION:')[1].strip()
            
            # Extract quality score
            score_line = [line for line in lines if line.startswith('QUALITY_SCORE:')][0]
            score_text = score_line.split('QUALITY_SCORE:')[1].strip()
            quality_score = None if score_text == "N/A" else float(score_text)
            
            # Extract explanation
            explanation_line = [line for line in lines if line.startswith('EXPLANATION:')]
            explanation = explanation_line[0].split('EXPLANATION:')[1].strip() if explanation_line else ""
            
            return classification, quality_score, explanation
            
        except (IndexError, Exception) as e:
            # Fallback parsing - look for refusal codes in the text
            response_text = response.upper()
            for code in self.valid_refusal_codes:
                if code in response_text:
                    return code, None, "Detected refusal code in response (fallback parsing)"
            
            return "answer_attempt", None, f"Could not parse response, defaulting to answer attempt. Error: {str(e)}"

    def evaluate_refusal_match(self, predicted_refusal_code, ground_truth_refusal_code):
        """
        Evaluate refusal category match (exact match only - no LLM judge needed).
        
        Args:
            predicted_refusal_code: Model's predicted refusal code
            ground_truth_refusal_code: Ground truth refusal code
            
        Returns:
            Tuple of (is_correct, explanation)
        """
        if predicted_refusal_code == ground_truth_refusal_code:
            return True, f"Exact match: {predicted_refusal_code}"
        else:
            return False, f"Mismatch: predicted={predicted_refusal_code}, ground_truth={ground_truth_refusal_code}"

    async def process_example(self, entry):
        """Process a single example across specified models with dual evaluation logic."""
        
        # Use specified models or all models
        self.model_id
        
        # Extract data - use RefusalBench dataset field names
        context = entry.get('perturbed_context', '')  # Use perturbed_context, not retrieved_docs[0]
        query = entry.get('perturbed_query', '')      # Use perturbed_query, not query
        ground_truth_label = entry.get('expected_rag_behavior', '')  # Use expected_rag_behavior, not ground_truth_label
        correct_answer = entry.get('original_answers', "") if ground_truth_label == "ANSWER_CORRECTLY" else None
        
        # Extract additional key fields for provenance
        unique_id = entry.get('unique_id', '')
        generator_model = entry.get('generator_model', '')
        perturbation_class = entry.get('perturbation_class', '')
        intensity = entry.get('intensity', '')
        
            
        # Evaluate each specified model
            
        try:
            # Get model prediction
            model_raw_output = await self.get_model_answer(self.model_id, context, query)
            
            # Combined classification and evaluation in one LLM call
            model_predicted_type, answer_quality_score, evaluation_explanation = await self.classify_and_evaluate_response(
                model_output=model_raw_output,
                query=query,
                correct_answers=correct_answer
            )
            
            # Initialize result structure with key additional fields
            result = {
                'model_id': self.model_id,
                'generator_model': generator_model,
                'perturbation_class': perturbation_class,
                'intensity': intensity,
                'query': query,
                'ground_truth_label': ground_truth_label,
                'ground_truth_answer': correct_answer if correct_answer else None,
                'model_raw_output': model_raw_output,
                'model_predicted_type': model_predicted_type,
                'answer_quality_score': None,
                'refusal_match_correct': None,
                'llm_evaluation_explanation': evaluation_explanation
            }
            
            # DUAL EVALUATION LOGIC
            if ground_truth_label == "ANSWER_CORRECTLY":
                # Answerable instance
                if model_predicted_type == "answer_attempt":
                    # Both GT and model provide answers - use quality score from combined evaluation
                    result['answer_quality_score'] = answer_quality_score
                    result['refusal_match_correct'] = None  # N/A for answer attempts
                else:
                    # GT expects answer but model refused - False Refusal
                    result['answer_quality_score'] = None  # N/A
                    result['refusal_match_correct'] = False  # Always wrong to refuse when should answer
                    result['llm_evaluation_explanation'] = f"False Refusal: Model refused with {model_predicted_type} when should have answered"
            else:
                # Unanswerable instance - GT is a refusal code
                if model_predicted_type == "answer_attempt":
                    # GT expects refusal but model answered - Missed Refusal
                    result['answer_quality_score'] = None  # N/A
                    result['refusal_match_correct'] = False  # Always wrong to answer when should refuse
                    result['llm_evaluation_explanation'] = f"Missed Refusal: Model answered when should have refused with {ground_truth_label}"
                else:
                    # Both GT and model are refusal codes - check exact match
                    is_correct, explanation = self.evaluate_refusal_match(model_predicted_type, ground_truth_label)
                    result['answer_quality_score'] = None  # N/A for refusals
                    result['refusal_match_correct'] = is_correct
                    result['llm_evaluation_explanation'] = explanation
        
            
        except Exception as e:
            logger.error(f"Error evaluating {self.model_id}: {e}")
            # Add a failed result to keep track
            result = {
                'model_id': self.model_id,
                'generator_model': generator_model,
                'perturbation_class': perturbation_class,
                'intensity': intensity,
                'query': query,
                'ground_truth_label': ground_truth_label,
                'ground_truth_answer': correct_answer if correct_answer else None,
                'model_raw_output': f"ERROR: {str(e)}",
                'model_predicted_type': "ERROR",
                'answer_quality_score': None,
                'refusal_match_correct': None,
                'llm_evaluation_explanation': "Evaluation failed due to error"
            }

        return result